{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOTION train 2840\n",
      "GOTION test 1419\n",
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m---> 31\u001b[0m f_g \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaSOH\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPPA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_prompt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, f_g\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39mLR)\n\u001b[0;32m     33\u001b[0m set_seed(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\gpt4battery\\models\\llama.py:34\u001b[0m, in \u001b[0;36mLlamaSOH.__init__\u001b[1;34m(self, input_dim, llama_model_path, PPA, soft_prompt_len)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder \u001b[38;5;241m=\u001b[39m PositionalEncoding(\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Load the pre-trained LLama 7B model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# self.llama = LlamaModel.from_pretrained('huggyllama/llama-7b',\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#             trust_remote_code=True,\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#             local_files_only=False,\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#             config=LlamaConfig.from_pretrained('huggyllama/llama-7b'),)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_linear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4096\u001b[39m, input_dim)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Freeze LLama parameters except for LayerNorm and positional encodings\u001b[39;00m\n",
      "File \u001b[1;32md:\\Applications\\Anaconda\\envs\\batteryML\\lib\\site-packages\\transformers\\modeling_utils.py:3788\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3782\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   3783\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   3784\u001b[0m )\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   3787\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 3788\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   3791\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32md:\\Applications\\Anaconda\\envs\\batteryML\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:845\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 845\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m LlamaRotaryEmbedding(config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[1;32md:\\Applications\\Anaconda\\envs\\batteryML\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:845\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 845\u001b[0m     [\u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;241m=\u001b[39m LlamaRotaryEmbedding(config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[1;32md:\\Applications\\Anaconda\\envs\\batteryML\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:634\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[1;34m(self, config, layer_idx)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LLAMA_ATTENTION_CLASSES[config\u001b[38;5;241m.\u001b[39m_attn_implementation](config\u001b[38;5;241m=\u001b[39mconfig, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx)\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[1;32md:\\Applications\\Anaconda\\envs\\batteryML\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:231\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mintermediate_size\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmlp_bias)\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmlp_bias)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mhidden_act]\n",
      "File \u001b[1;32md:\\Applications\\Anaconda\\envs\\batteryML\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 180355072 bytes."
     ]
    }
   ],
   "source": [
    "from main_pretrains.main_gpt2 import main_pretrain\n",
    "from models.llama import LlamaSOH\n",
    "from data_provider import data_provider\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "source_set_train, source_loader_train = data_provider(name = 'GOTION', shuffle_flag=True, batch_size=10, flag = 'train')\n",
    "source_set_test, source_loader_test = data_provider(name = 'GOTION', shuffle_flag=False, batch_size=10, flag = 'test')\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 1\n",
    "llama_model_path = './llms/llama'\n",
    "\n",
    "# Training parameters\n",
    "mask_ratio = 0.3\n",
    "num_epochs = 15\n",
    "LR = 1e-3\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "f_g = LlamaSOH(input_dim, llama_model_path, PPA=True, soft_prompt_len=10).to(device)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, f_g.parameters()), lr=LR)\n",
    "set_seed(0)\n",
    "# Assuming train_loader and test_loader are already defined\n",
    "main_pretrain(train_loader= source_loader_train, test_loader= source_loader_test, model=f_g, optimizer=optimizer, device=device, mask_ratio=mask_ratio, num_epochs=num_epochs)\n",
    "torch.save(f_g.state_dict(), 'saved_models/llama/f_g.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prob (PG-SSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_pretrains.main_gpt2_prob import main_prob\n",
    "import torch.nn as nn\n",
    "\n",
    "target_set_train, target_loader_train = data_provider(name = 'CALCE', shuffle_flag=False, batch_size=10, flag = 'train')\n",
    "target_set_test, target_loader_test = data_provider(name = 'CALCE', shuffle_flag=False, batch_size=10, flag = 'test')\n",
    "\n",
    "LR = 1e-3\n",
    "EPOCH = 3\n",
    "\n",
    "f_g = GPT2SOH(input_dim, gpt2_model_path, PPA=True, soft_prompt_len=10).to(device)\n",
    "f_g.load_state_dict(torch.load(\"saved_models/gpt2+ppa/f_g.pt\"))\n",
    "\n",
    "regressor = nn.Linear(f_g.input_linear.out_features, 1).to(device)\n",
    "optimizer = optim.AdamW(regressor.parameters(), lr=LR)\n",
    "\n",
    "set_seed(2)\n",
    "main_prob(source_loader_train, target_loader_train, combined_training=True, model=f_g, regressor=regressor, optimizer=optimizer, device=device, num_epochs=EPOCH)\n",
    "torch.save(regressor.state_dict(), 'saved_models/gpt2+ppa/regressor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drawings import drawDegradation\n",
    "drawDegradation(target_loader_test=target_loader_test, model=f_g, regressor=regressor, device = device, model_name = 'gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tta (PPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_pretrains.main_gpt2_tta import main_tta\n",
    "from evaluator import evaluate\n",
    "\n",
    "f_g = GPT2SOH(input_dim, gpt2_model_path, PPA=True, soft_prompt_len=10).to(device)\n",
    "f_g.load_state_dict(torch.load(\"saved_models/gpt2+ppa/f_g.pt\"))\n",
    "regressor = nn.Linear(f_g.input_linear.out_features, 1).to(device)\n",
    "regressor.load_state_dict(torch.load(\"saved_models/gpt2+ppa/regressor.pt\"))\n",
    "\n",
    "mae, rmse = evaluate(model = f_g, regressor=regressor, target_loader=target_loader_test, device=device, model_name='gpt2')\n",
    "print(f\"(Before TTA) MAE: {mae}, RMSE: {rmse}\")\n",
    "\n",
    "# 冻结GPT-2模型的所有参数\n",
    "# for param in f_g.gpt2.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "for name, param in f_g.gpt2.named_parameters():\n",
    "    if not any(layer in name.lower() for layer in ['ln', 'wpe', 'wte']):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Print trainable and frozen parameters\n",
    "print(\"Trainable parameters:\")\n",
    "trainable_params_count = 0\n",
    "for name, param in f_g.named_parameters():\n",
    "    if name =='soft_prompt':\n",
    "        print('PPA params:', param.numel())\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        trainable_params_count += param.numel()\n",
    "\n",
    "print(\"\\nFrozen parameters:\")\n",
    "frozen_params_count = 0\n",
    "for name, param in f_g.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(name)\n",
    "        frozen_params_count += param.numel()\n",
    "\n",
    "print(f\"\\nNumber of trainable parameters: {trainable_params_count}\")\n",
    "print(f\"Total number of parameters: {trainable_params_count + frozen_params_count}\")\n",
    "print(f\"Number of frozen parameters: {frozen_params_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b optimizer = optim.SGD(filter(lambda p: p.requires_grad, f_g.parameters()), lr=1e-2)\n",
    "\n",
    "optimizer = optim.SGD([f_g.soft_prompt], lr=1e-1, momentum=0.9)\n",
    "set_seed(2)\n",
    "mae, rmse = evaluate(model = f_g, regressor=regressor, target_loader=target_loader_test, device=device, model_name='gpt2')\n",
    "print(f\"(Before TTA) MAE: {mae}, RMSE: {rmse}\")\n",
    "\n",
    "main_tta(target_loader=target_loader_test, model=f_g, regressor=regressor, optimizer=optimizer, device=device, mask_ratio=0.9, num_epochs=10)\n",
    "\n",
    "mae, rmse = evaluate(model = f_g, regressor=regressor, target_loader=target_loader_test, device=device, model_name='gpt2')\n",
    "print(f\"(After TTA) MAE: {mae}, RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batteryML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
