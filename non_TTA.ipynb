{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.other_models import MLP_SOH, RandomForestSOH, LightGBMSOH\n",
    "from evaluator import evaluate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, target_loader, combined_training, device, model_type='mlp', num_epochs=100, lr=1e-3):\n",
    "    if model_type == 'mlp':\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        target_data_iter = iter(target_loader)\n",
    "        target_batch = next(target_data_iter)\n",
    "        src_tar = target_batch[0].to(device)\n",
    "        labels_tar = target_batch[1].to(device)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for i, batch in enumerate(train_loader):\n",
    "\n",
    "                if combined_training:\n",
    "                    src = batch[0].to(device)\n",
    "                    src = torch.cat((src, src_tar), dim=0)\n",
    "                    labels = batch[1].to(device)\n",
    "                    labels = torch.cat((labels, labels_tar), dim=0)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(src).squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                else:\n",
    "                    src = batch[0].to(device)\n",
    "                    labels = batch[1].to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(src).squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "            epoch_loss /= len(train_loader)\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.8f}')\n",
    "\n",
    "        model.eval()\n",
    "        test_labels = []\n",
    "        test_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(target_loader):\n",
    "                src = batch[0].to(device)\n",
    "                labels = batch[1].to(device)\n",
    "                outputs = model(src).squeeze()\n",
    "                test_labels.extend(labels.cpu().numpy())\n",
    "                test_predictions.extend(outputs.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    elif model_type == 'random_forest':\n",
    "        train_data = np.concatenate([batch[0].cpu().numpy().reshape(batch[0].size(0), -1) for batch in train_loader])\n",
    "        train_labels = np.concatenate([batch[1].cpu().numpy() for batch in train_loader])\n",
    "\n",
    "        model.fit(train_data, train_labels)\n",
    "\n",
    "        test_data = np.concatenate([batch[0].cpu().numpy().reshape(batch[0].size(0), -1) for batch in target_loader])\n",
    "        test_labels = np.concatenate([batch[1].cpu().numpy() for batch in target_loader])\n",
    "        test_predictions = model.predict(test_data)\n",
    "\n",
    "    elif model_type == 'lightgbm':\n",
    "        train_data = np.concatenate([batch[0].cpu().numpy().reshape(batch[0].size(0), -1) for batch in train_loader])\n",
    "        train_labels = np.concatenate([batch[1].cpu().numpy() for batch in train_loader])\n",
    "\n",
    "        train_dataset = lgb.Dataset(train_data, train_labels)\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt'\n",
    "        }\n",
    "        model = lgb.train(params, train_dataset, num_boost_round=100)\n",
    "\n",
    "        test_data = np.concatenate([batch[0].cpu().numpy().reshape(batch[0].size(0), -1) for batch in target_loader])\n",
    "        test_labels = np.concatenate([batch[1].cpu().numpy() for batch in target_loader])\n",
    "        test_predictions = model.predict(test_data, num_iteration=model.best_iteration)\n",
    " \n",
    "    test_labels = np.array(test_labels)\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(test_labels, test_predictions))\n",
    "    mae = mean_absolute_error(test_labels, test_predictions)\n",
    "    print(f'Test RMSE: {rmse:.4f}')\n",
    "    print(f'Test MAE: {mae:.4f}')\n",
    "    return rmse, mae\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOTION train 2840\n",
      "GOTION test 1419\n",
      "SANYO train 404\n",
      "SANYO test 8\n",
      "cuda\n",
      "Epoch 1/10, Loss: 0.02863636\n",
      "Epoch 2/10, Loss: 0.00230913\n",
      "Epoch 3/10, Loss: 0.00211875\n",
      "Epoch 4/10, Loss: 0.00175806\n",
      "Epoch 5/10, Loss: 0.00136597\n",
      "Epoch 6/10, Loss: 0.00087639\n",
      "Epoch 7/10, Loss: 0.00043379\n",
      "Epoch 8/10, Loss: 0.00016542\n",
      "Epoch 9/10, Loss: 0.00005383\n",
      "Epoch 10/10, Loss: 0.00002177\n",
      "Test RMSE: 0.0610\n",
      "Test MAE: 0.0593\n"
     ]
    }
   ],
   "source": [
    "from data_provider import data_provider\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some dummy data\n",
    "    np.random.seed(42)\n",
    "    source_set_train, source_loader_train = data_provider(name = 'GOTION', shuffle_flag=True, batch_size=10, flag = 'train')\n",
    "    source_set_test, source_loader_test = data_provider(name = 'GOTION', shuffle_flag=False, batch_size=10, flag = 'test')\n",
    "    target_set_train, target_loader_train = data_provider(name = 'SANYO', shuffle_flag=False, batch_size=10, flag = 'train')\n",
    "    target_set_test, target_loader_test = data_provider(name = 'SANYO', shuffle_flag=False, batch_size=10, flag = 'test')\n",
    "\n",
    "    # Train MLP model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    mlp_model = MLP_SOH(input_dim=1, hidden_dim=64, num_layers=2).to(device)\n",
    "    train_and_evaluate(mlp_model, source_loader_train, target_loader_train, combined_training=False,  device=device, model_type='mlp', num_epochs=10,  lr=1e-3)\n",
    "\n",
    "    # # Train Random Forest model\n",
    "    # rf_model = RandomForestSOH(n_estimators=100, random_state=42)\n",
    "    # train_and_evaluate(rf_model, X, y, model_type='rf')\n",
    "\n",
    "    # # Train LightGBM model\n",
    "    # lgb_model = LightGBMSOH(num_leaves=31, learning_rate=0.05, n_estimators=100)\n",
    "    # train_and_evaluate(lgb_model, X, y, model_type='lgb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4353"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Test RMSE: 0.0787\n",
      "Test MAE: 0.0673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.07869677744070395, 0.06732322864234447)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestSOH(n_estimators=100)\n",
    "print(\"Training Random Forest...\")\n",
    "train_and_evaluate(rf_model, source_loader_train, target_loader_test, combined_training=False, device=device, model_type='random_forest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12240\n",
      "[LightGBM] [Info] Number of data points in the train set: 2840, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 0.875610\n",
      "Test RMSE: 0.0631\n",
      "Test MAE: 0.0562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06311573870314201, 0.05624137769777611)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define LightGBM model\n",
    "lgb_model = None  # Placeholder\n",
    "print(\"Training LightGBM...\")\n",
    "train_and_evaluate(lgb_model, source_loader_train, source_loader_test, combined_training=False, device=device, model_type='lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'booster_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_params\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate parameters\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m params_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_lgb_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlgb_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightGBM model parameter count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[63], line 19\u001b[0m, in \u001b[0;36mcount_lgb_params\u001b[1;34m(lgb_model)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_lgb_params\u001b[39m(lgb_model):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Count the total number of parameters in a LightGBM model.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mlgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbooster_\u001b[49m\n\u001b[0;32m     20\u001b[0m     model_info \u001b[38;5;241m=\u001b[39m booster\u001b[38;5;241m.\u001b[39mdump_model()\n\u001b[0;32m     21\u001b[0m     total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'booster_'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def count_lgb_tree_params(tree_structure):\n",
    "    \"\"\"\n",
    "    Recursively count the number of nodes in a tree.\n",
    "    Each node in the tree represents a parameter.\n",
    "    \"\"\"\n",
    "    if isinstance(tree_structure, dict):\n",
    "        num_nodes = 1  # Count the current node\n",
    "        num_nodes += count_lgb_tree_params(tree_structure.get('left_child', {}))\n",
    "        num_nodes += count_lgb_tree_params(tree_structure.get('right_child', {}))\n",
    "        return num_nodes\n",
    "    return 0\n",
    "\n",
    "def count_lgb_params(lgb_model):\n",
    "    \"\"\"\n",
    "    Count the total number of parameters in a LightGBM model.\n",
    "    \"\"\"\n",
    "    booster = lgb_model.booster_\n",
    "    model_info = booster.dump_model()\n",
    "    total_params = 0\n",
    "    num_trees = len(model_info['tree_info'])\n",
    "    for tree in model_info['tree_info']:\n",
    "        total_params += count_lgb_tree_params(tree['tree_structure'])\n",
    "    return total_params\n",
    "\n",
    "\n",
    "# Calculate parameters\n",
    "params_count = count_lgb_params(lgb_model)\n",
    "print(f\"LightGBM model parameter count: {params_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batteryML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
